---
phase: 10-log-viewer-tab
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - tools/gsd-review-broker/src/gsd_review_broker/dashboard.py
  - tools/gsd-review-broker/tests/test_dashboard.py
autonomous: true
requirements: [LOGS-01, LOGS-02]

must_haves:
  truths:
    - "API returns list of JSONL log files from both broker-logs/ and reviewer-logs/ with name, size, and modification time"
    - "API returns parsed JSONL entries for a selected log file"
    - "SSE streams new log entries as they are written to disk for a subscribed file"
  artifacts:
    - path: "tools/gsd-review-broker/src/gsd_review_broker/dashboard.py"
      provides: "Log listing, log file reading, and SSE log tail endpoints"
      exports: ["register_dashboard_routes"]
    - path: "tools/gsd-review-broker/tests/test_dashboard.py"
      provides: "Tests for log API endpoints"
  key_links:
    - from: "dashboard.py /dashboard/api/logs"
      to: "broker-logs/ and reviewer-logs/ directories on disk"
      via: "_default_user_config_dir() path resolution"
      pattern: "_resolve_broker_log_dir|_resolve_reviewer_log_dir"
    - from: "dashboard.py /dashboard/api/logs/{filename}"
      to: "JSONL files on disk"
      via: "Path resolution with directory containment check"
      pattern: "relative_to"
    - from: "dashboard.py SSE log_tail events"
      to: "JSONL file on disk"
      via: "File tail polling with seek/tell"
      pattern: "log_tail"
---

<objective>
Add Python backend API endpoints and SSE streaming for log file browsing and live tail.

Purpose: Provides the data layer for the Log Viewer tab -- listing available log files, reading their contents, and streaming new entries in real-time via SSE.

Output: Three new API routes in dashboard.py (/dashboard/api/logs, /dashboard/api/logs/{filename}, SSE log_tail events) with tests.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/10-log-viewer-tab/10-CONTEXT.md
@.planning/phases/08-dashboard-shell-and-infrastructure/08-02-SUMMARY.md
@.planning/phases/09-overview-tab/09-01-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add log listing and log file reading API endpoints</name>
  <files>tools/gsd-review-broker/src/gsd_review_broker/dashboard.py</files>
  <action>
Add two new HTTP endpoints to `register_dashboard_routes()` in dashboard.py. Register them AFTER /dashboard/api/overview but BEFORE the /dashboard and /dashboard/{path:path} catch-all routes (route registration order matters per established pattern).

**Endpoint 1: GET /dashboard/api/logs**

Returns a JSON list of all JSONL log files from both broker-logs/ and reviewer-logs/ directories.

Implementation:
- Reuse `_default_user_config_dir()` from server.py's pattern (already exists in pool.py). Import or duplicate the resolution logic -- since dashboard.py already has its own module-level helpers, add two small private helpers:
  - `_resolve_broker_log_dir() -> Path` returns `_default_user_config_dir() / "broker-logs"` (check BROKER_LOG_DIR env var override first, matching server.py pattern)
  - `_resolve_reviewer_log_dir() -> Path` returns `_default_user_config_dir() / "reviewer-logs"` (check BROKER_REVIEWER_LOG_DIR env var override first, matching pool.py pattern)
  - Add a `_default_user_config_dir() -> Path` helper in dashboard.py that resolves the cross-platform user config dir (XDG_CONFIG_HOME, APPDATA on Windows, ~/Library/Application Support on macOS, ~/.config on Linux) for the "gsd-review-broker" dirname. This duplicates the logic from server.py and pool.py to avoid importing from those modules (same pattern as _query_review_stats duplicating SQL from tools.py).
- For each directory, if it exists, list all `*.jsonl` files (including rotated files like `broker.jsonl.1`, `broker.jsonl.2` etc -- use glob pattern `*.jsonl*` to catch both `.jsonl` and `.jsonl.N` files).
- For each file, collect: `name` (filename string), `size` (bytes as int), `modified` (ISO 8601 UTC timestamp string from file mtime), `source` ("broker" or "reviewer" based on parent directory).
- Sort files by modification time descending (most recent first).
- Return JSON: `{ "files": [...] }`
- If directories don't exist, return empty list (not an error).

**Endpoint 2: GET /dashboard/api/logs/{filename}**

Returns the parsed contents of a specific JSONL log file.

Implementation:
- Accept `filename` path parameter.
- Security: resolve the filename against BOTH log directories (try broker-logs/ first, then reviewer-logs/). Use `Path.relative_to()` for directory containment check (established pattern from dashboard_static). Reject path traversal attempts with 404.
- Read the entire file (per user decision: "Load full file contents when switching files").
- Parse each line as JSON. Skip lines that fail to parse (malformed entries from crashes/rotation).
- Return JSON: `{ "filename": "broker.jsonl", "source": "broker", "entries": [...], "size": filesize_bytes }`
- Each entry in entries is the raw parsed JSON object from the JSONL line (preserving all fields: ts, level, logger, caller_tag, message, event, reviewer_id, session_token, stream, pid, exit_code, exception as present).
- Return 404 if file not found in either directory.
  </action>
  <verify>
Run `uv run pytest tests/test_dashboard.py -v` from tools/gsd-review-broker/ -- existing tests still pass plus new log endpoint tests (added in Task 2).

Manual verification: the two new route handlers exist in dashboard.py with proper path containment security.
  </verify>
  <done>
/dashboard/api/logs returns file listing from both log directories. /dashboard/api/logs/{filename} returns parsed JSONL entries with path traversal protection. Both endpoints handle missing directories gracefully.
  </done>
</task>

<task type="auto">
  <name>Task 2: Add SSE log tail streaming and log endpoint tests</name>
  <files>
    tools/gsd-review-broker/src/gsd_review_broker/dashboard.py
    tools/gsd-review-broker/tests/test_dashboard.py
  </files>
  <action>
**SSE Log Tail Enhancement:**

Modify the existing `dashboard_events` SSE endpoint in dashboard.py to support log file tail streaming. The SSE endpoint already pushes `overview_update` events. Add log tail functionality alongside it:

- Add a query parameter `?tail={filename}` to the /dashboard/events SSE endpoint. When present, the SSE stream includes `log_tail` events for new entries written to that file.
- Implementation approach: after each heartbeat interval sleep, if a tail filename is active:
  1. Resolve the filename to a path in broker-logs/ or reviewer-logs/ (same lookup as /dashboard/api/logs/{filename}, with path containment security).
  2. Track the file position using `seek(0, SEEK_END)` on first connection to get initial size, then on each interval check if file size has grown.
  3. If file has grown, read new bytes from the previous position to current end, split into lines, parse each as JSON.
  4. Yield SSE events: `data: {"type": "log_tail", "entries": [...]}\n\n` where entries is the array of new parsed JSONL objects.
  5. If no new entries, skip (don't send empty events).
  6. Handle file rotation gracefully: if file size shrinks (rotation occurred), reset position to 0 and re-read from start.
  7. Handle file not found: if the tail file doesn't exist (yet), just skip and retry next interval.
- Use a shorter polling interval for log tail (2 seconds) vs the existing overview heartbeat interval (15 seconds). Restructure the SSE loop to check log tail every 2s and overview every 15s (use a counter or separate timing).
- Continue sending overview_update events at the existing 15s interval alongside log tail events.

**Tests:**

Add new test functions to test_dashboard.py:

1. `test_log_listing_empty` -- API returns empty list when log directories don't exist.
2. `test_log_listing_with_files` -- Create temp JSONL files in mock broker-logs/ and reviewer-logs/ dirs, set env vars (BROKER_LOG_DIR, BROKER_REVIEWER_LOG_DIR) to point to temp dirs, call the endpoint, verify response includes files from both sources with correct name/size/source/modified fields.
3. `test_log_file_read` -- Create a temp JSONL file with 3 sample entries, call /dashboard/api/logs/{filename}, verify entries are parsed correctly.
4. `test_log_file_read_not_found` -- Request a nonexistent file, verify 404.
5. `test_log_file_path_traversal` -- Request `../../etc/passwd`, verify 404 (path containment blocks it).
6. `test_sse_log_tail` -- Test SSE with `?tail=broker.jsonl` parameter. Create a temp JSONL file, set env var, invoke SSE handler directly (per established pattern from 08-02 -- test via direct handler invocation and body_iterator consumption). Verify log_tail events appear in the stream. Write a new entry to the file mid-stream and verify it appears.

For test fixtures: use `tmp_path` pytest fixture for temporary directories. Set BROKER_LOG_DIR and BROKER_REVIEWER_LOG_DIR env vars via `monkeypatch.setenv()` to redirect log resolution to temp dirs during tests. Use the established `overview_ctx` fixture pattern for app context setup.

Sample JSONL test data (matching real broker log format):
```json
{"ts":"2026-02-26T12:00:00.000Z","level":"info","logger":"gsd_review_broker","caller_tag":"broker","message":"Server started on 0.0.0.0:8321"}
{"ts":"2026-02-26T12:00:01.000Z","level":"info","logger":"gsd_review_broker","caller_tag":"proposer","message":"Review created: abc-123"}
```

Sample reviewer log test data:
```json
{"ts":"2026-02-26T12:00:00.000Z","event":"reviewer_output","reviewer_id":"reviewer-1","session_token":"sess-1","stream":"stderr","message":"Starting review session","pid":12345}
```
  </action>
  <verify>
Run `uv run pytest tests/test_dashboard.py -v` from tools/gsd-review-broker/ -- all existing tests pass plus 6 new log-related tests pass.
  </verify>
  <done>
SSE endpoint supports ?tail={filename} parameter streaming new log entries every 2s. All 6 new tests pass covering file listing, file reading, path traversal protection, and SSE log tail streaming. Existing dashboard tests (overview, static serving) continue to pass.
  </done>
</task>

</tasks>

<verification>
1. `uv run pytest tests/test_dashboard.py -v` -- all tests pass (existing + 6 new)
2. Log listing endpoint returns files from both broker-logs/ and reviewer-logs/ directories
3. Log file reading endpoint parses JSONL and returns structured entries
4. Path traversal is blocked on log file reading endpoint
5. SSE log tail streams new entries when file grows on disk
</verification>

<success_criteria>
- /dashboard/api/logs returns file listing with name, size, modified, source for all JSONL files
- /dashboard/api/logs/{filename} returns parsed entries from the file with path traversal protection
- SSE ?tail={filename} streams new log entries in real-time as they are written
- All tests pass including new log-specific tests
</success_criteria>

<output>
After completion, create `.planning/phases/10-log-viewer-tab/10-01-SUMMARY.md`
</output>
